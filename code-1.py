# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_1Ji9aNqI3ZQfZQMdyLRgZP7fsAW5uUs
"""

# code
import numpy as np
import pandas as pd
import sklearn
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

ratings = pd.read_csv("/content/ratings (1).csv")
ratings.head()

advertisements = pd.read_csv("/content/OnlineRetail1.csv", encoding= 'unicode_escape')
advertisements.head()

n_ratings = len(ratings)
n_advertisements = len(ratings['ad_id'].unique())
n_users = len(ratings['userId'].unique())

print(f"Number of ratings: {n_ratings}")
print(f"Number of unique ad_id's: {n_advertisements}")
print(f"Number of unique users: {n_users}")
print(f"Average ratings per user: {round(n_ratings/n_users, 2)}")
print(f"Average ratings per advertisement: {round(n_ratings/n_advertisements, 2)}")

user_freq = ratings[['userId', 'ad_id']].groupby('userId').count().reset_index()
user_freq.columns = ['userId', 'n_ratings']
user_freq.head()


# Find Lowest and Highest rated advertisements:
mean_rating = ratings.groupby('ad_id')[['rating']].mean()
# Lowest rated advertisements
lowest_rated = mean_rating['rating'].idxmin()
advertisements.loc[advertisements['ad_id'] == lowest_rated]
# Highest rated advertisements
highest_rated = mean_rating['rating'].idxmax()
advertisements.loc[advertisements['ad_id'] == highest_rated]
# show number of people who rated advertisements rated advertisement highest
ratings[ratings['ad_id']==highest_rated]
# show number of people who rated advertisements rated advertisement lowest
ratings[ratings['ad_id']==lowest_rated]

## the above advertisements has very low dataset. We will use bayesian average
advertisement_stats = ratings.groupby('ad_id')[['rating']].agg(['count', 'mean'])
advertisement_stats.columns = advertisement_stats.columns.droplevel()

# Now, we create user-item matrix using scipy csr matrix
from scipy.sparse import csr_matrix

def create_matrix(df):
	
	N = len(df['userId'].unique())
	M = len(df['ad_id'].unique())
	
	# Map Ids to indices
	user_mapper = dict(zip(np.unique(df["userId"]), list(range(N))))
	advertisement_mapper = dict(zip(np.unique(df["ad_id"]), list(range(M))))
	
	# Map indices to IDs
	user_inv_mapper = dict(zip(list(range(N)), np.unique(df["userId"])))
	advertisement_inv_mapper = dict(zip(list(range(M)), np.unique(df["ad_id"])))
	
	user_index = [user_mapper[i] for i in df['userId']]
	advertisement_index = [advertisement_mapper[i] for i in df['ad_id']]

	X = csr_matrix((df["rating"], (advertisement_index, user_index)), shape=(M, N))
	
	return X, user_mapper, advertisement_mapper, user_inv_mapper, advertisement_inv_mapper

X, user_mapper, advertisement_mapper, user_inv_mapper, advertisement_inv_mapper = create_matrix(ratings)

from sklearn.neighbors import NearestNeighbors
"""
Find similar advertisements using KNN
"""
def find_similar_advertisements(advertisement_id, X, k, metric='cosine', show_distance=False):
	
	neighbour_ids = []
	
	advertisement_ind = advertisement_mapper[advertisement_id]
	advertisement_vec = X[advertisement_ind]
	k+=1
	kNN = NearestNeighbors(n_neighbors=k, algorithm="brute", metric=metric)
	kNN.fit(X)
	advertisement_vec = advertisement_vec.reshape(1,-1)
	neighbour = kNN.kneighbors(advertisement_vec, return_distance=show_distance)
	for i in range(0,k):
		n = neighbour.item(i)
		neighbour_ids.append(advertisement_inv_mapper[n])
	neighbour_ids.pop(0)
	return neighbour_ids


advertisement_titles = dict(zip(advertisements['ad_id'], advertisements['title']))


advertisement_id = 10

similar_ids = find_similar_advertisements(advertisement_id, X, k=10)
advertisement_title = advertisement_titles[advertisement_id]

print(f"Since you watched {advertisement_title}")
for i in similar_ids:
	print(advertisement_titles[i])





"""# New Section"""